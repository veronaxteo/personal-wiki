{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25bd41a9",
   "metadata": {},
   "source": [
    "# bert vs gpt-2: an analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c6b881",
   "metadata": {},
   "source": [
    "A comparison of BERT vs GPT-2.\n",
    "\n",
    "After working with both models and hearing about why some choose to use one over the other in particular situations, I thought I would do a quick analysis of the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c0e9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install transformers\n",
    "# !pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7bbc230",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OpenAIGPTTokenizer, GPT2LMHeadModel, GPT2Tokenizer, BertTokenizer, BertForMaskedLM, logging\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import tensor\n",
    "from torch.nn.functional import softmax\n",
    "from time import time\n",
    "\n",
    "# logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b838ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wikitext (/Users/veronateo/.cache/huggingface/datasets/wikitext/wikitext-2-v1/1.0.0/a241db52902eaf2c6aa732210bead40c090019a499ceb13bcbfa3f8ab646a126)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbedebb93aec4fdfb546c06e65fce84a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "wikitext2 = load_dataset('wikitext', 'wikitext-2-v1')\n",
    "wikitext2 = [x['text'].strip() for x in wikitext2['test']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "657b7da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sequences with random length from 1-100\n",
    "def get_gpt2_sequences(data, batch_size=256, seq_len=100):\n",
    "    response = []\n",
    "        \n",
    "    tokenized = [tokenizer(x)['input_ids'] for x in data]\n",
    "    tokenized = [x for x in tokenized if len(x) >= seq_len]\n",
    "        \n",
    "    while len(response) < batch_size:\n",
    "        # pick a random line\n",
    "        line = tokenized[randint(len(tokenized))]\n",
    "\n",
    "        # get random start position\n",
    "        start = 0 if len(line) == seq_len else randint(len(line) - seq_len)\n",
    "        end = start + seq_len\n",
    "        \n",
    "        window = line[start:end]\n",
    "        predict_index = randint(seq_len)\n",
    "        replaced = window[predict_index]\n",
    "        window = window[0:predict_index]\n",
    "        \n",
    "        if len(window) == 0:\n",
    "            continue\n",
    "        \n",
    "        window, attention_mask = pad_sequence(window, seq_len)\n",
    "        \n",
    "        r = {\n",
    "            'context_length': predict_index,\n",
    "            'window': window,\n",
    "            'replaced_token': replaced,\n",
    "            'attention_mask': attention_mask\n",
    "        }\n",
    "        \n",
    "        response.append(r)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d485b0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_sequences(data, batch_size=256, seq_len=100):\n",
    "    MASK_TOKEN = 103\n",
    "    response = []\n",
    "\n",
    "    tokenized = [tokenizer(x)['input_ids'] for x in data]\n",
    "    tokenized = [x for x in tokenized if len(x) >= seq_len]\n",
    "    \n",
    "    while len(response) < batch_size:\n",
    "    # pick a random line\n",
    "        line = tokenized[randint(len(tokenized))]\n",
    "\n",
    "        start = 0 if len(line) == seq_len else randint(len(line) - seq_len)\n",
    "        end = start + seq_len\n",
    "\n",
    "        window = line[start:end]\n",
    "        predict_index = randint(seq_len)\n",
    "        actual_token = window[predict_index]\n",
    "        window[predict_index] = MASK_TOKEN\n",
    "\n",
    "        r = {\n",
    "          'window': window,\n",
    "          'replaced_token': actual_token,\n",
    "          'position': predict_index\n",
    "        }\n",
    "\n",
    "        response.append(r)\n",
    "    \n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "248ec098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_gpt2(batch, model):\n",
    "    actual = [x['replaced_token'] for x in batch]\n",
    "    context_lengths = [x['context_length'] for x in batch]\n",
    "    context = { 'input_ids': tensor([x['window'] for x in batch]),\n",
    "                'attention_mask': tensor([x['attention_mask'] for x in batch])\n",
    "                }\n",
    "    with torch.no_grad():\n",
    "        predicted = model.generate(**context, max_length=1)\n",
    "        predicted = [x[-1].item() for x in predicted]\n",
    "    \n",
    "    return predicted, actual, context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "429f9b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_bert(batch, model):\n",
    "    actual = [x['replaced_token'] for x in batch]\n",
    "    positions = [x['position'] for x in batch]\n",
    "    position_mask = tensor([[x] for x in positions]).to('cuda')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # predictions = bert(input_ids=tensor([x['window'] for x in batch]).to('cuda'))\n",
    "        predictions = bert(input_ids=tensor([x['window'] for x in batch]))\n",
    "        predictions = predictions[0].argmax(dim=-1)\n",
    "        predictions = [x.item() for x in predictions.gather(1, position_mask)]\n",
    "\n",
    "    return predictions, actual, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66224bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sequence(sequence, length=100, pad_token=50256):\n",
    "    pad_len = length - len(sequence)\n",
    "    padding = [pad_token] * pad_len\n",
    "    attention_mask = [0] * pad_len + [1 for x in sequence]\n",
    "    \n",
    "    padded = padding + sequence\n",
    "    return padded, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d127443a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, sequence_getter, predict_fn, batch_size=256):\n",
    "    all_predicted, all_actual, all_context_lengths = [], [], []\n",
    "    epoch = 0\n",
    "    \n",
    "    while len(all_predicted) < 10_000:\n",
    "        epoch += 1\n",
    "        batch = sequence_getter(wikitext2, batch_size)\n",
    "        predicted, actual, context_lengths = predict_fn(batch, model)\n",
    "        all_predicted += predicted\n",
    "        all_actual += actual\n",
    "        all_context_lengths += context_lengths\n",
    "\n",
    "        print('\\r', f'epoch {epoch}\\trunning accuracy: {np.mean([1 if x==y else 0 for x,y in zip(all_predicted, all_actual)])}', end='')\n",
    "\n",
    "    return all_predicted, all_actual, all_context_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2766636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## gpt2 ##########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 1\trunning accuracy: 0.32421875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 2\trunning accuracy: 0.31640625"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 3\trunning accuracy: 0.3216145833333333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 4\trunning accuracy: 0.3193359375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 5\trunning accuracy: 0.32109375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 6\trunning accuracy: 0.3216145833333333"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 7\trunning accuracy: 0.3247767857142857"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 8\trunning accuracy: 0.31982421875"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 9\trunning accuracy: 0.3233506944444444"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 10\trunning accuracy: 0.32109375"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch 11\trunning accuracy: 0.3203125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 1. This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    }
   ],
   "source": [
    "evaluation = {}\n",
    "\n",
    "gpt_models = [('gpt2', 256), ('gpt2-medium', 128)]\n",
    "\n",
    "for model_name, batch_size in gpt_models:\n",
    "    print(f'{\"#\"*10} {model_name} {\"#\"*10}')\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    # model = GPT2LMHeadModel.from_pretrained(model_name).to('cuda')\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    predicted, actual, lengths = evaluate(model, get_gpt2_sequences, predict_gpt2, batch_size)\n",
    "    evaluation[model_name] = {'predicted': predicted, 'actual': actual, 'pos': lengths}\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a60f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_models = [('bert-base-cased', 256), ('bert-base-cased', 256), ('bert-large-cased', 128), ('bert-large-uncased', 128)]\n",
    "\n",
    "for model_name, batch_size in bert_models:\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    model = BertForMaskedLM.from_pretrained(model_name).to('cuda')\n",
    "    predicted, actual, mask_positions = evaluate(model, get_bert_sequences, predict_bert)\n",
    "    evaluation[model_name] = {'predicted':predicted, 'actual':actual, 'mask_positions': mask_positions}\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d980218",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = {}\n",
    "models = evaluation.keys()\n",
    "for m in models:\n",
    "    model_data = data[m]\n",
    "    model_type = data[m].pop('type', None)\n",
    "    model_accuracy = [[] for x in range(100)]\n",
    "\n",
    "    for pred, act, length in [a for a in zip(*model_data.values())]:\n",
    "        model_accuracy[length].append(1 if pred == act else 0)\n",
    "    accuracies[m] = [np.mean(x) if len(x) > 0 else 0 for x in model_accuracy]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe302343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('eval.json', 'w') as f:\n",
    "    json.dump(accuracies, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
